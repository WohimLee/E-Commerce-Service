## 测试评估

```
n=25 elapsed=59.3s qps=0.42 mean_lat=2370.9ms
```



### 一、当前的评估

#### 1 每一项是什么意思？

##### 1️⃣ `n=25`

> **已经评测了 25 条 query**

* 这是“query 级”的数量
* 如果的 JSONL 里：

  ```json
  {"skuid": 123, "questions": ["q1", "q2", "q3"]}
  ```

  那这 3 条都会算进 `n`



##### 2️⃣ `elapsed=59.3s`

> **从开始评测到现在，一共跑了 59.3 秒**

* 包含：

  * embedding
  * ES BM25
  * ES kNN
  * Python RRF 融合
* 是**真实墙钟时间（wall time）**



##### 3️⃣ `qps=0.42`

> **当前平均吞吐 ≈ 0.42 query / 秒**

计算方式是：

```
qps = n / elapsed = 25 / 59.3 ≈ 0.42
```

等价说法：

* **≈ 每 2.4 秒跑 1 个 query**
* 这是**离线评测 + embedding + 双路检索**的典型量级


##### 4️⃣ `mean_lat=2370.9ms`

> **单条 query 的平均延迟 ≈ 2.37 秒**

这是：

```python
mean_lat = mean(time(search(q)))
```

包括：

* embedding（模型前向）
* BM25 ES search
* kNN ES search
* Python 融合



#### 2 这个速度“正常吗”？要不要慌？

##### 结论先给：

> **在现在的 setup 下：这是完全正常的**

我们对照一下：

| 场景                                         | 延迟          |
| ------------------------------------------ | ----------- |
| 只 BM25                                     | 20–50 ms    |
| 向量检索（GPU embed + ES kNN）                   | 200–500 ms  |
| **现在：embedding + BM25 + kNN + Python 融合** | **2–3 s** ✅ |

原因很明确👇



#### 3 为什么会这么慢？（根因分析）

##### 🔥 最大头号原因：**embedding 在循环里反复算**

现在是：

```python
for q in queries:
    query_vector = embedder.embed([q])[0]   # 👈 每次都跑模型
```

如果的 embedder 是：

* CPU
* 或者大模型（BGE / M3 / multilingual）

那 **1 次 forward ≈ 100~300ms（CPU 更慢）**

25 次 ≈ 25 × 200ms = 5 秒
但现在是 **2.3 秒 / 次**，说明还有下面这些 👇



##### 🔥 第二原因：**ES kNN 本身就慢**

* `num_candidates=1500`
* `k=200`
* 双路检索

这是**离线评测参数**，不是线上参数
👉 正常、但慢



##### 🔥 第三原因：**是串行的**

```python
for q in queries:
    search(q)
```

* 没并发
* 没 batch
* 没缓存



#### 4 现在应该怎么解读这行日志？

> 它不是在告诉“系统不行”，而是在告诉：

* ✅ **评测在正常跑**
* ⏳ **这是离线评测速度，不是线上 SLA**
* 📊 **这个速度是有优化空间的，但不是现在必须做**

---

#### 5 如果现在想“快一点”（3 个实用档位）

##### 🟢 档位 1（最推荐，5 分钟搞定）

**只跑一部分评测**

```python
# 在 iter_queries_from_jsonl 里临时加
if n > 100: break
```

先看 Hit@10 / MRR 趋势即可


##### 🟡 档位 2（中期，性价比最高）

**缓存 query embedding**

```python
embed_cache = {}

def get_qvec(q):
    if q not in embed_cache:
        embed_cache[q] = embedder.embed([q])[0]
    return embed_cache[q]
```

👉 立刻减少 30~50% 时间（如果 query 有重复）



##### 🔵 档位 3（后期，线上化）

* embedding 服务化（GPU + batch）
* search 并发
* ES rerank / LTR



### 二、QPS 400-500

很多人嘴里说的 “**QPS 400–500**”，跟现在看到的这个 `qps=0.42` 不是同一个东西


##### 一句话先给结论

> **别人说的 “QPS 400–500”**
> 👉 通常指的是：
> **一个“已部署的线上接口服务”，在稳定状态下，每秒能处理 400–500 个请求**

而现在的：

```
qps=0.42
```

👉 是 **离线评测脚本 + 串行 + embedding + ES 检索** 的吞吐
👉 **完全不是同一类指标**


#### 1 先把 QPS 的“语境”分清楚（非常重要）

##### 现在的 QPS（0.42）属于哪一类？

现在的 QPS 是：

> **Offline / 单进程 / 串行 / 冷启动 embedding 的评测 QPS**

特点：

* 单进程
* 单线程
* 每个 query：

  * 重新算 embedding
  * 调 ES 两次
* 没 batch
* 没并发
* 没缓存

👉 这是 **“最慢形态”** 的 QPS
👉 目的只是 **评测效果，不是压性能**



##### 一般说的 “QPS 400–500” 指的是什么？

通常指的是下面这个模型👇

```
[客户端并发请求]
        ↓
[HTTP Search API]
        ↓
[已 warm 的服务]
        ↓
[ES / 向量库 / cache]
```

关键前提：

| 条件                  | 是否成立 |
| ------------------- | ---- |
| 并发请求                | ✅    |
| embedding 已缓存 / 已算好 | ✅    |
| GPU batch           | ✅    |
| 服务常驻内存              | ✅    |
| 无 Python for-loop   | ✅    |

👉 **这是线上吞吐能力**

---

##### 用一个表帮对齐认知

| 场景                       | QPS 量级          |
| ------------------------ | --------------- |
| 现在的离线评测脚本               | **0.3 – 1**     |
| 单进程 + 轻量 BM25            | 20 – 50         |
| 向量检索（embedding 已算）       | 100 – 300       |
| **线上检索 API（并发 + cache）** | **300 – 1000+** |
| 高配 GPU + ANN             | 2000+           |

所以：

> ❌ **不能拿现在的 0.42 QPS 去对比别人说的 400 QPS**
> ✅ 那相当于拿“单人手推车”对比“高速公路吞吐量”



#### 2 “QPS 400–500”通常隐含了哪些条件？

当一个工程师说：

> “我们检索服务能扛 **400–500 QPS**”

他通常隐含的是：

##### 1. **HTTP 层并发**
```text
50–200 并发连接
```
##### 2. **embedding 已完成**

* query embedding 要么：

  * 在上游算
  * 要么 batch GPU 算
  * 要么 cache 命中

##### 3. **ES / 向量库 warm**

* segment 已 cache
* filter 命中率高

##### 4. **P99 延迟可控**
  ```text
  p50 ≈ 20–40ms
  p99 ≈ 100–200ms
  ```

👉 这才配谈 QPS 400–500



#### 3 给一个直觉换算（很重要）

>当前的情况

```
mean_lat ≈ 2370ms
```

理论最大 QPS（单线程）：

```
1 / 2.37 ≈ 0.42 QPS   ✅（和看到的一致）
```

>如果是“线上服务”

假设：

* mean_lat = 50ms
* 并发 = 50

那么：

```
QPS ≈ 并发 / latency
    ≈ 50 / 0.05
    = 1000 QPS
```

👉 **QPS = 并发 × (1 / 延迟)**


###  三、现在应该怎么“正确看 QPS”？

#### 1 对现在的阶段（stage1）

应该关注的是：

* ✅ Hit@K / MRR
* ❌ 不用在意 QPS
* ❌ 不要跟别人线上 QPS 比


#### 2 到什么时候才该关心 QPS？

当做到：

* embedding 服务化
* search API HTTP 化
* 并发压测（wrk / hey / locust）

这时关心的才是：

```text
p50 / p95 / p99 latency
QPS @ SLA
```
