## 检索精度

- **Hit@K ≈ Recall@K（等价）**
* **MRR** = *Mean Reciprocal Rank*，衡量「**第一个正确结果排得有多靠前**」


### 1️⃣ Hit@K 和 Recall@K：是不是一回事？

#### 标准定义（学术上）

* **Recall@K**
  $$
  \text{Recall@K} = \frac{\text{在前 K 个结果中被召回的相关文档数}}{\text{该 query 的相关文档总数}}
  $$

* **Hit@K（Hit Rate）**
$$
\mathrm{Hit} @ \mathrm{~K}= \begin{cases}1 & \text { 前 } \mathrm{K} \text { 个结果中至少有一个相关文档 } \\ 0 & \text { 否则 }\end{cases}
$$


#### 在评测代码里发生了什么？

评测逻辑是（简化）：

```python
# 每个 query 只有一个 target_id
if rank is not None and rank <= k:
    hit = 1
else:
    hit = 0
```

也就是说：

* **每个 query 只有 1 个“正确答案”**
* 分母永远是 1

因此：

$$
\text { Recall@K }= \begin{cases}1 & \text { 找到了这个 target } \\ 0 & \text { 没找到 }\end{cases}
$$

⬇️**这和 Hit@K 完全一样**


#### ⚠️ 什么时候不一样？

当 **一个 query 有多个正确答案** 时：

| 场景              | Hit@K | Recall@K |
| --------------- | ----- | -------- |
| K=10 找到 1 个相关文档 | 1     | 0.2      |
| K=10 找到 3 个相关文档 | 1     | 0.6      |

👉 现在的评测不是这种情况。



### 2️⃣ 那 MRR 是什么？

#### 定义：Mean Reciprocal Rank

##### 单个 query 的 RR（Reciprocal Rank）：

$$
R R= \begin{cases}\frac{1}{\operatorname{rank}} & \text { 第一个相关结果排在 rank } \\ 0 & \text { 一个都没找到 }\end{cases}
$$

**MRR** 是所有 query 的平均：

$$
MRR = \frac{1}{N} \sum_{i=1}^{N} RR_i
$$



##### 用直觉理解

* 命中在 **第 1 名** → RR = **1.0**
* 命中在 **第 2 名** → RR = **0.5**
* 命中在 **第 5 名** → RR = **0.2**
* **Top K 内没命中** → RR = **0**

👉 **越早出现正确结果，MRR 越高**


#### 代码

这里：

```python
stats_map[k].add_one(True, 1.0 / rank)
```

所以：

* Hit@K：**“有没有”**
* MRR@K：**“排得靠不靠前”**

> 两个模型 Hit@10 都是 0.90，但：
>
> * 模型 A：正确结果多在第 1–2 位 → MRR 高
> * 模型 B：正确结果多在第 8–10 位 → MRR 低

---

### 3️⃣ 在现在的搜索评测中，三个指标怎么用？

#### 现在的指标含义

| 指标       | 回答的问题                      |
| -------- | -------------------------- |
| Hit@K    | 用户翻到第 K 条以内，**能不能看到正确商品？** |
| Recall@K | （在这里）同 Hit@K              |
| MRR@K    | 正确商品**通常排第几名？**            |



#### 典型解读方式（很实用）

* **Hit@20 很高，但 MRR@20 很低**
  👉 召回还行，但排序差（正确商品常在第 10–20 位）
* **Hit@5 低，Hit@20 高**
  👉 召回慢，TopK 不够锋利
* **MRR 提升但 Hit 不变**
  👉 排序优化有效（rerank / RRF / embedding 质量）



### 4️⃣ 如果以后想更严格一点

可以考虑：

* 多 target（一个 query 对多个 sku）
* 再加一个：

  * **NDCG@K**（考虑多个相关性等级）
  * 或 **Recall@K（multi-hit）**
